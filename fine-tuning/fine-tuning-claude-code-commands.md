# Claude Code Commands — Fine-Tune Embedding Model for Improved Search Relevancy

## How to Use

Open Claude Code in your JetBrains IDE. Paste each prompt **one at a time, in order**. Wait for completion before moving on. Each prompt includes an **"Interview Explanation"** section so you can articulate exactly what's happening and why.

> **Prerequisites:** Python 3.10+, pip, an Anthropic API key (for synthetic data generation), and the Java service already refactored to delegate embeddings to Python (see java-service-option-a-commands.md)

### Why This Lives Inside ~/llm-inference/

Since the Java service now delegates all embedding generation to the Python service, the Python service is the **single source of truth** for the model. The fine-tuning project lives inside the same directory so that:

- The fine-tuned model is saved right next to the service that loads it
- Updating `app.py` to load the fine-tuned model is a one-line change
- Rebuilding the Docker image automatically includes the fine-tuned model
- Everything ships together: service code, model weights, training scripts

```
~/llm-inference/
├── app.py                          ← Python inference service (loads the model)
├── Dockerfile
├── requirements.txt
├── ingestion-worker/               ← Existing ingestion worker
└── embedding-finetune/             ← NEW: fine-tuning project (we're creating this)
    ├── requirements.txt
    ├── config.py
    ├── 01_extract_products.py
    ├── ...
    ├── data/
    └── fine-tuned-all-MiniLM-L6-v2-amazon/   ← Output: the fine-tuned model
```

---

## PROMPT 1 — Project Setup and Dependencies

**Interview Explanation:** *Before fine-tuning, I need to set up a Python project with the key libraries. Sentence Transformers v3 is the library that wraps HuggingFace transformers with a high-level training API specifically designed for embedding models. The `datasets` library provides efficient data loading that integrates with the trainer, and `anthropic` is used to call Claude for synthetic training data generation. I'm placing this inside the llm-inference directory because the Python inference service owns the embedding model — after the Java service refactor, it's the single source of truth. The fine-tuned model will be saved right next to the service code that loads it.*

```
Create a new directory called "embedding-finetune" inside ~/llm-inference/. Inside it, create a requirements.txt with these exact dependencies:

sentence-transformers>=3.0
datasets>=2.19
transformers>=4.41
accelerate>=0.31
torch>=2.1
anthropic>=0.25
psycopg2-binary>=2.9
pandas>=2.0
numpy>=1.24
scikit-learn>=1.3

Then run: pip install -r requirements.txt

Also create an empty __init__.py and the following subdirectory structure:
~/llm-inference/embedding-finetune/
├── requirements.txt
├── config.py
├── 01_extract_products.py
├── 02_generate_training_data.py
├── 03_baseline_evaluation.py
├── 04_fine_tune.py
├── 05_evaluate_improvement.py
├── 06_re_embed_products.py
├── 07_compare_search_results.py
└── data/          (empty directory for generated files)

For config.py, create it with these constants:

import os

# Database (your existing RDS PostgreSQL)
DB_HOST = os.getenv("DB_HOST", "llm-postgres.cgd6mmmueuhm.us-east-1.rds.amazonaws.com")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "llmdb")
DB_USER = os.getenv("DB_USERNAME", "postgres")
DB_PASS = os.getenv("DB_PASSWORD", "")

# Embedding model
BASE_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
FINETUNED_MODEL_DIR = os.path.join(os.path.dirname(__file__), "fine-tuned-all-MiniLM-L6-v2-amazon")
EMBEDDING_DIM = 384

# Training
TRAIN_EPOCHS = 3
BATCH_SIZE = 32
LEARNING_RATE = 2e-5
WARMUP_RATIO = 0.1
QUERIES_PER_PRODUCT = 4

# Paths
DATA_DIR = "./data"
TRAINING_DATA_PATH = os.path.join(DATA_DIR, "training_pairs.json")
TRAIN_SPLIT_PATH = os.path.join(DATA_DIR, "train_split.json")
TEST_SPLIT_PATH = os.path.join(DATA_DIR, "test_split.json")
PRODUCTS_PATH = os.path.join(DATA_DIR, "products.json")
BASELINE_RESULTS_PATH = os.path.join(DATA_DIR, "baseline_results.json")
FINETUNED_RESULTS_PATH = os.path.join(DATA_DIR, "finetuned_results.json")

Print the installed sentence-transformers version to confirm it's v3+.
```

---

## PROMPT 2 — Extract Products from PostgreSQL

**Interview Explanation:** *The first step is to extract the product data from our existing PostgreSQL database. These are the same 1,013 Amazon products that were ingested through our S3 → Lambda → SQS → ECS ingestion pipeline. Each record has a title, description, searchable_content (the concatenated text that was embedded), and the existing 384-dimensional embeddings generated by the base all-MiniLM-L6-v2 model. I need this data both as the corpus for evaluation and as the source material for generating synthetic training queries.*

```
Create 01_extract_products.py that:

1. Connects to the PostgreSQL database using credentials from config.py
2. Queries the ingested_records table:

   SELECT id, title, description, category, tags, searchable_content, source_file
   FROM ingested_records
   WHERE status = 'active'
     AND content_embedding IS NOT NULL
   ORDER BY id

3. Stores each row as a dictionary in a list. For the tags field (PostgreSQL TEXT[]), convert it to a Python list. Handle None values gracefully.

4. Saves the full list to data/products.json

5. Prints summary stats:
   - Total products extracted
   - Number of unique categories
   - Average length of searchable_content (in characters and approximate tokens by dividing by 4)
   - Sample of 3 random products showing id, title, category

6. Also prints the first product's searchable_content so we can see the exact format that was embedded (this is critical because our training data needs to match this format)

Use psycopg2 for the database connection. Handle the case where DB_PASS might be empty and the user needs to set it via environment variable — print a clear error message.

The script should be runnable as: python 01_extract_products.py
```

---

## PROMPT 3 — Generate Synthetic Training Data with Claude

**Interview Explanation:** *This is the most important conceptual step. We don't have real user search logs, which is the typical training signal for improving search relevancy. Instead, I use an LLM (Claude) to generate synthetic search queries — the kinds of things a real customer would type into a search box when looking for each product. For each of the 1,013 products, I generate 4 diverse queries. This gives me approximately 4,000 (query, relevant_product) positive pairs. The key insight is that these pairs teach the embedding model what "relevance" means in our specific product domain. The base model was trained on general internet text and doesn't understand that "budget-friendly gift for a runner" should strongly match "adidas Racer Tr21 Running Shoe." Fine-tuning on these pairs moves the embedding space so that product-relevant queries land closer to their target products.*

```
Create 02_generate_training_data.py that:

1. Loads products from data/products.json

2. For each product, calls the Anthropic API (claude-sonnet-4-20250514) with this system prompt:

   "You generate realistic Amazon search queries. Return ONLY the queries, one per line, no numbering, no extra text."

   And this user prompt (formatted with the product's actual data):

   """Given this Amazon product, generate {QUERIES_PER_PRODUCT} different search queries a real customer might type when looking for this product.

   Product Title: {title}
   Description: {description}
   Category: {category}

   Requirements:
   - Each query should be 3-8 words, like real search box input
   - Include variety: one specific query with brand/feature details, one general need-based query, one natural language query ("something for..."), one attribute-focused query
   - Do NOT copy the product title verbatim
   - Think about what PROBLEM the customer is solving, not just what the product IS"""

3. For each generated query, create a training pair:
   {
     "query": "the generated query text",
     "positive": "the product's searchable_content field (this is what was actually embedded in PostgreSQL)",
     "product_id": the product's database id,
     "title": "the product title (for reference only)"
   }

4. Include retry logic: if an API call fails, retry up to 3 times with exponential backoff. If it still fails, skip that product and log a warning.

5. Save ALL pairs to data/training_pairs.json

6. Then split 80/20 into data/train_split.json and data/test_split.json using random shuffling with a fixed seed (42) for reproducibility.

7. Print progress every 50 products: "Generated queries for 50/1013 products (198 pairs so far)"

8. Print final summary:
   - Total training pairs generated
   - Train split size / Test split size
   - 5 sample pairs showing the query and the first 80 chars of the product text
   - Estimated API cost (input tokens + output tokens)

IMPORTANT: Use max_tokens=200 for each API call to keep costs low. The entire generation for 1,013 products should cost roughly $1-2.

The script should be runnable as: python 02_generate_training_data.py
It requires the ANTHROPIC_API_KEY environment variable to be set.
```

---

## PROMPT 4 — Baseline Evaluation (Before Fine-Tuning)

**Interview Explanation:** *Before I change anything, I need to measure how well the current base model performs so I have a quantitative baseline to compare against. I use the InformationRetrievalEvaluator from the sentence-transformers library, which is the standard tool for this. It works like this: I give it a set of test queries, a corpus of all products, and a mapping of which queries should match which products (the "ground truth" relevance judgments). It then embeds all queries and all corpus documents, computes cosine similarity between every query and every document, ranks the results, and calculates standard information retrieval metrics.*

*The key metric is NDCG@10 — Normalized Discounted Cumulative Gain at rank 10. NDCG is the industry-standard metric for search ranking quality. Unlike simple accuracy which just checks "is the right answer in the top-10?", NDCG also considers POSITION. If the correct product is ranked #1, that scores higher than if it's ranked #8, because in a real search UI position matters enormously. The "discounted" part applies a logarithmic penalty to lower positions. The "normalized" part scales the score to 0-1 so we can compare across different query sets.*

*I also measure MRR@10 (Mean Reciprocal Rank) — the average of 1/rank for the first correct result. If MRR@10 is 0.5, the first relevant product typically appears around position 2. And Recall@k — what fraction of relevant products appear in the top-k results.*

```
Create 03_baseline_evaluation.py that:

1. Loads the test split from data/test_split.json and products from data/products.json

2. Loads the base model: SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

3. Builds the three data structures that InformationRetrievalEvaluator requires:

   corpus: dict mapping corpus_id (string) -> document text
     - For EVERY product in products.json (not just test products), add an entry:
       corpus[str(product["id"])] = product["searchable_content"]
     - This is critical: the evaluator searches across the ENTIRE corpus of 1,013 products, not just the test set. This makes the retrieval task realistic.

   queries: dict mapping query_id (string) -> query text
     - For each pair in the test split:
       queries[f"q{i}"] = pair["query"]

   relevant_docs: dict mapping query_id -> set of relevant corpus_ids
     - For each pair in the test split:
       relevant_docs[f"q{i}"] = {str(pair["product_id"])}
     - Each query has exactly one relevant product (the one it was generated from)

4. Creates an InformationRetrievalEvaluator with:
   - queries, corpus, relevant_docs as defined above
   - name="amazon-products-baseline"
   - mrr_at_k=[1, 5, 10]
   - ndcg_at_k=[5, 10]
   - accuracy_at_k=[1, 3, 5, 10]
   - precision_recall_at_k=[5, 10]
   - score_functions={"cosine": cos_sim}  (import cos_sim from sentence_transformers.util)
   - main_score_function="cosine"
   - show_progress_bar=True
   - batch_size=64

5. Runs the evaluator: results = evaluator(model)

6. Prints a formatted results table:

   ============================================================
   BASELINE EVALUATION — all-MiniLM-L6-v2 (before fine-tuning)
   ============================================================
   Corpus size:    1,013 products
   Test queries:   {N} queries
   
   RANKING QUALITY:
     NDCG@5:       {value:.4f}
     NDCG@10:      {value:.4f}    ← PRIMARY METRIC
   
   FIRST RELEVANT RESULT:
     MRR@1:        {value:.4f}
     MRR@5:        {value:.4f}
     MRR@10:       {value:.4f}
   
   HIT RATE (correct product in top-k):
     Accuracy@1:   {value:.4f}    (top result is correct X% of the time)
     Accuracy@3:   {value:.4f}
     Accuracy@5:   {value:.4f}
     Accuracy@10:  {value:.4f}
   
   COVERAGE:
     Recall@5:     {value:.4f}
     Recall@10:    {value:.4f}
   ============================================================

7. Saves the full results dict to data/baseline_results.json for later comparison

8. Also runs 5 example queries manually to show what the baseline retrieval looks like:

   For each of these hardcoded queries, encode the query, encode all product searchable_content fields, compute cosine similarity, and show the top-3 results with similarity scores:

   test_queries = [
       "gift for someone who loves cooking",
       "comfortable running shoes for men",
       "budget friendly electronics for kids",
       "waterproof jacket for hiking in rain",
       "wireless noise cancelling headphones",
   ]

   Print them like:
   Query: "gift for someone who loves cooking"
     1. [0.412] Premium Kitchen Knife Set - Professional...
     2. [0.389] Silicone Cooking Utensils Set...
     3. [0.351] Cast Iron Dutch Oven...

The script should be runnable as: python 03_baseline_evaluation.py
```

---

## PROMPT 5 — Fine-Tune the Embedding Model

**Interview Explanation:** *Now I fine-tune the actual embedding model. The key components are:*

*1. The TRAINING DATA FORMAT: I have positive pairs — (query, relevant_product_text). Sentence Transformers expects a dataset with columns named "anchor" and "positive".*

*2. The LOSS FUNCTION: I use MultipleNegativesRankingLoss (also called InfoNCE or in-batch negatives). This is the standard loss for training retrieval models with only positive pairs. Here's how it works: for a batch of 32 query-product pairs, the loss treats all OTHER products in the batch as negatives for each query. So for query_1, the correct product is product_1, but product_2 through product_32 are treated as negatives. This gives us 31 negatives for free without having to mine them separately. The loss pushes the query embedding closer to its positive product and farther from all the batch negatives in the embedding space. Larger batch sizes give more negatives per query, which generally improves learning.*

*3. The TRAINING PROCESS: The SentenceTransformerTrainer is a subclass of HuggingFace's Trainer. It handles the training loop: forward pass (embed query + product), compute loss, backpropagation, gradient update. I train for 3 epochs with learning rate 2e-5 and warmup ratio 0.1 (the learning rate linearly increases from 0 to 2e-5 during the first 10% of training steps, then linearly decays). The evaluator runs after each epoch so I can track improvement.*

*4. WHY THIS WORKS: The base model already understands general English semantics. Fine-tuning with domain-specific pairs performs "domain adaptation" — it adjusts the embedding space so that product-domain concepts cluster correctly. The model learns that "budget-friendly" relates to low prices, "gift for a runner" relates to running shoes, and "something waterproof for hiking" relates to outdoor gear. We're not training from scratch — we're nudging an already-capable model.*

```
Create 04_fine_tune.py that:

1. Loads the base model: SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

2. Loads training data from data/train_split.json and converts to a HuggingFace Dataset:
   
   train_dataset = Dataset.from_dict({
       "anchor": [pair["query"] for pair in train_data],
       "positive": [pair["positive"] for pair in train_data],
   })

   Print: "Training dataset: {len} pairs"
   Print: "Sample pair:"
   Print: "  Query:   '{first query}'"
   Print: "  Product: '{first 100 chars of product text}...'"

3. Builds the evaluation infrastructure (same as baseline script):
   - Load test split and all products
   - Build corpus dict (ALL 1,013 products)
   - Build queries dict and relevant_docs dict from test split
   - Create InformationRetrievalEvaluator with same settings as the baseline script
   - Name it "amazon-products-finetune"

4. Runs baseline evaluation FIRST to capture the starting point:

   print("Evaluating baseline before fine-tuning...")
   baseline_results = evaluator(model)
   baseline_ndcg = baseline_results["cosine_ndcg@10"]
   print(f"Baseline NDCG@10: {baseline_ndcg:.4f}")

5. Creates the loss function:

   from sentence_transformers.losses import MultipleNegativesRankingLoss
   loss = MultipleNegativesRankingLoss(model)

6. Configures training arguments:

   args = SentenceTransformerTrainingArguments(
       output_dir="./training-checkpoints",
       num_train_epochs=3,
       per_device_train_batch_size=32,
       learning_rate=2e-5,
       warmup_ratio=0.1,
       fp16=False,  # set True if CUDA GPU available
       eval_strategy="epoch",
       save_strategy="epoch",
       logging_steps=25,
       save_total_limit=2,
       load_best_model_at_end=True,
       metric_for_best_model="amazon-products-finetune_cosine_ndcg@10",
       report_to="none",  # disable wandb/tensorboard for simplicity
   )

7. Creates and runs the trainer:

   trainer = SentenceTransformerTrainer(
       model=model,
       args=args,
       train_dataset=train_dataset,
       loss=loss,
       evaluator=evaluator,
   )

   Print: "Starting fine-tuning: {epochs} epochs, batch size {bs}, lr {lr}"
   Print: "Training steps per epoch: ~{len(train_dataset) // batch_size}"
   Print: "Total training steps: ~{total}"

   trainer.train()

8. After training, runs final evaluation:

   print("Evaluating fine-tuned model...")
   finetuned_results = evaluator(model)
   finetuned_ndcg = finetuned_results["cosine_ndcg@10"]
   print(f"Fine-tuned NDCG@10: {finetuned_ndcg:.4f}")

9. Saves the fine-tuned model:

   model.save("fine-tuned-all-MiniLM-L6-v2-amazon")
   print("Model saved to: fine-tuned-all-MiniLM-L6-v2-amazon/")

10. Prints a summary:

   print(f"NDCG@10: {baseline_ndcg:.4f} → {finetuned_ndcg:.4f} ({finetuned_ndcg - baseline_ndcg:+.4f})")

11. Saves both result sets to data/finetuned_results.json as:
   {
     "baseline": baseline_results,
     "finetuned": finetuned_results
   }

The script should be runnable as: python 04_fine_tune.py
Expected runtime on MacBook M-series: 10-20 minutes (CPU only, no GPU needed).
```

---

## PROMPT 6 — Comprehensive Evaluation and Comparison Report

**Interview Explanation:** *This script produces the quantitative evidence that fine-tuning worked. It loads both the baseline and fine-tuned results and creates a comparison across every metric. The most important number is the NDCG@10 improvement — this is what I'd cite first in an interview. But the other metrics tell a complementary story: MRR tells you where the first correct result typically appears (position 1 vs position 3), Accuracy@1 tells you how often the very first result is correct, and Recall@10 tells you whether the correct product appears anywhere in the top 10. Together they paint a complete picture of search quality improvement.*

```
Create 05_evaluate_improvement.py that:

1. Loads both the base model and the fine-tuned model:
   
   base_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
   fine_model = SentenceTransformer("fine-tuned-all-MiniLM-L6-v2-amazon")

2. Loads test split and all products. Builds the evaluator data structures (corpus, queries, relevant_docs) exactly as in the previous scripts.

3. Creates TWO evaluators (one per model, different names for clarity):
   - "baseline" evaluator
   - "finetuned" evaluator
   (They use the same corpus, queries, and relevant_docs but will evaluate different models)

4. Runs both evaluations:

   print("Evaluating base model...")
   base_results = base_evaluator(base_model)
   
   print("Evaluating fine-tuned model...")
   fine_results = fine_evaluator(fine_model)

5. Prints a comprehensive comparison table:

   ╔══════════════════════════════════════════════════════════════════╗
   ║        EMBEDDING FINE-TUNING RESULTS — SEARCH RELEVANCY        ║
   ╠══════════════════════════════════════════════════════════════════╣
   ║                                                                 ║
   ║  Base Model:      sentence-transformers/all-MiniLM-L6-v2       ║
   ║  Fine-Tuned on:   ~4,000 synthetic (query, product) pairs      ║
   ║  Loss Function:   MultipleNegativesRankingLoss (in-batch neg)  ║
   ║  Training:        3 epochs, batch=32, lr=2e-5                  ║
   ║  Corpus:          1,013 Amazon products                        ║
   ║  Test Queries:    {N} queries                                  ║
   ║                                                                 ║
   ╠═════════════════╦═══════════╦═══════════╦═══════════╦══════════╣
   ║  Metric         ║  Baseline ║ Fine-Tuned║   Delta   ║  Change  ║
   ╠═════════════════╬═══════════╬═══════════╬═══════════╬══════════╣
   ║  NDCG@5         ║  0.XXXX  ║  0.XXXX   ║  +0.XXXX  ║  +X.X%   ║
   ║  NDCG@10 ★      ║  0.XXXX  ║  0.XXXX   ║  +0.XXXX  ║  +X.X%   ║
   ║  MRR@10         ║  0.XXXX  ║  0.XXXX   ║  +0.XXXX  ║  +X.X%   ║
   ║  Accuracy@1     ║  0.XXXX  ║  0.XXXX   ║  +0.XXXX  ║  +X.X%   ║
   ║  Accuracy@5     ║  0.XXXX  ║  0.XXXX   ║  +0.XXXX  ║  +X.X%   ║
   ║  Accuracy@10    ║  0.XXXX  ║  0.XXXX   ║  +0.XXXX  ║  +X.X%   ║
   ║  Recall@5       ║  0.XXXX  ║  0.XXXX   ║  +0.XXXX  ║  +X.X%   ║
   ║  Recall@10      ║  0.XXXX  ║  0.XXXX   ║  +0.XXXX  ║  +X.X%   ║
   ╚═════════════════╩═══════════╩═══════════╩═══════════╩══════════╝
   
   ★ = Primary metric

6. Then runs 5 SIDE-BY-SIDE search comparisons. For each test query below, embed with BOTH models, compute cosine similarity against ALL product texts, and show top-5 from each:

   comparison_queries = [
       "gift for someone who loves cooking",
       "comfortable running shoes for men",
       "budget friendly electronics for kids",
       "waterproof jacket for hiking in rain",
       "wireless noise cancelling headphones",
   ]

   For each query, print:

   Query: "gift for someone who loves cooking"
   ┌─────────────────────────────────────────┬─────────────────────────────────────────┐
   │  BASE MODEL                             │  FINE-TUNED MODEL                       │
   ├─────────────────────────────────────────┼─────────────────────────────────────────┤
   │  1. [0.412] Premium Kitchen Knife Set   │  1. [0.589] Premium Kitchen Knife Set   │
   │  2. [0.389] Silicone Cooking Utensil    │  2. [0.571] Silicone Cooking Utensil    │
   │  3. [0.351] Cast Iron Dutch Oven        │  3. [0.543] Stainless Steel Cookware    │
   │  4. [0.312] Bamboo Cutting Board        │  4. [0.521] Cast Iron Dutch Oven        │
   │  5. [0.298] Spice Rack Organizer        │  5. [0.498] Bamboo Cutting Board        │
   └─────────────────────────────────────────┴─────────────────────────────────────────┘
   Avg similarity improvement: +0.163 (base avg: 0.352, fine-tuned avg: 0.515)

7. Saves everything to data/comparison_report.json including the full metric tables and the side-by-side search examples.

8. At the very end, prints a one-line conclusion:
   
   "CONCLUSION: Fine-tuning improved NDCG@10 from {base} to {fine} ({pct}% improvement) using {N} synthetic training pairs."

The script should be runnable as: python 05_evaluate_improvement.py
```

---

## PROMPT 7 — Re-Embed All Products in PostgreSQL

**Interview Explanation:** *Once I've confirmed the fine-tuned model improves relevancy, I need to update the actual production embeddings in PostgreSQL. This means re-encoding all 1,013 products using the fine-tuned model and writing the new 384-dimensional vectors back to the content_embedding and title_embedding columns. The pgvector HNSW indexes will automatically update. After this, the live search API (both Python and Java services) will use the improved embeddings. The dimensionality stays at 384 so no schema changes are needed — just the vectors themselves change.*

```
Create 06_re_embed_products.py that:

1. Loads the fine-tuned model from the saved directory

2. Connects to PostgreSQL

3. Fetches all active records:
   SELECT id, title, searchable_content
   FROM ingested_records
   WHERE status = 'active'

4. Processes in batches of 64:
   - Encodes searchable_content with the fine-tuned model → new content_embedding
   - Encodes title with the fine-tuned model → new title_embedding
   - Updates each record:
     UPDATE ingested_records
     SET content_embedding = %s::vector,
         title_embedding = %s::vector
     WHERE id = %s

5. Converts the numpy float32 arrays to Python lists for psycopg2 (use .tolist())

6. Commits after each batch

7. Prints progress: "Re-embedded batch 64/1013 (6.3%)"

8. After all batches, runs a verification query:
   SELECT id, title,
          1 - (content_embedding <=> (SELECT content_embedding FROM ingested_records WHERE id = {first_id})::vector) as self_similarity
   FROM ingested_records
   WHERE id = {first_id}

   This should return similarity = 1.0 (or very close), confirming the embeddings were written correctly.

9. Prints total time taken and records updated.

IMPORTANT SAFETY: Before updating, ask the user for confirmation:
   "This will overwrite all 1,013 product embeddings in the live database."
   "The Java and Python search services will immediately use the new embeddings."
   "Type 'yes' to proceed: "

The script should be runnable as: python 06_re_embed_products.py
```

---

## PROMPT 8 — Live Search Comparison Against the API

**Interview Explanation:** *This final script tests the improvement end-to-end through the actual deployed API, not just locally in Python. It calls our live ALB endpoint with a set of search queries BEFORE and AFTER the re-embedding (I'd run it once before running script 06, save the results, then run it again after). This proves the improvement carries through the entire production stack — from the API endpoint, through embedding generation in the Python service (which the Java service also delegates to via the /embed endpoint), to pgvector cosine similarity search with HNSW indexing.*

```
Create 07_compare_search_results.py that:

1. Takes the ALB DNS as a command line argument or environment variable:
   ALB_DNS = os.getenv("ALB_DNS", "llm-alb-1402483560.us-east-1.elb.amazonaws.com")

2. Defines a list of test queries:
   test_queries = [
       "gift for someone who loves cooking",
       "comfortable running shoes for men",
       "budget friendly electronics for kids",
       "waterproof jacket for hiking in rain",
       "wireless noise cancelling headphones",
       "eco friendly reusable products for home",
       "back support cushion for office chair",
       "portable charger for camping trip",
       "toys for toddlers learning to walk",
       "professional camera for beginners",
   ]

3. For each query, calls the search API:
   POST http://{ALB_DNS}/api/search   (the Java endpoint)
   with body: {"query": query, "top_k": 5}

   If the Java endpoint isn't deployed, falls back to:
   POST http://{ALB_DNS}/search/records  (the Python endpoint)
   with body: {"query": query, "top_k": 5}

4. Collects results and prints a formatted table for each query:
   
   Query: "gift for someone who loves cooking"
   ┌────┬──────────────────────────────────────────────┬────────────┐
   │  # │  Product Title                               │ Similarity │
   ├────┼──────────────────────────────────────────────┼────────────┤
   │  1 │  Premium Kitchen Knife Set                   │    0.589   │
   │  2 │  Silicone Cooking Utensils Set               │    0.571   │
   │  3 │  Stainless Steel Cookware Set                │    0.543   │
   │  4 │  Cast Iron Dutch Oven                        │    0.521   │
   │  5 │  Bamboo Cutting Board                        │    0.498   │
   └────┴──────────────────────────────────────────────┴────────────┘

5. At the end, prints aggregate stats:
   - Average top-1 similarity across all queries
   - Average top-5 similarity across all queries
   - Number of queries where top-1 similarity > 0.5

6. Saves all results to data/live_search_results.json with a timestamp so results from before and after re-embedding can be compared manually.

7. If a file data/live_search_results_BEFORE.json exists, load it and print a comparison:

   "BEFORE re-embedding: avg top-1 similarity = 0.XXX"
   "AFTER re-embedding:  avg top-1 similarity = 0.XXX"
   "Improvement: +X.X%"

   Include instructions: "Run this script BEFORE re-embedding and save the output:
   cp data/live_search_results.json data/live_search_results_BEFORE.json
   Then run 06_re_embed_products.py, then run this script again to see the comparison."

The script should be runnable as: python 07_compare_search_results.py
```

---

## PROMPT 9 — Update Python Inference Service to Load the Fine-Tuned Model

**Interview Explanation:** *Since the Java service now delegates all embedding generation to the Python inference service, updating the model is a one-service change. I just need to modify app.py to load the fine-tuned model instead of the base model from HuggingFace. The fine-tuned model directory sits right next to app.py inside the llm-inference project, so I update the EMBED_MODEL_ID environment variable (or the default path) to point to the local fine-tuned model. The model has the same architecture and dimensionality — only the transformer weights are different — so no API changes, no schema changes, and the Java service automatically gets the improved embeddings through the /embed endpoint.*

```
This step updates the Python inference service (~/llm-inference/app.py) to load the fine-tuned model instead of the base HuggingFace model.

Two changes are needed:

CHANGE 1: Update app.py to support loading from a local directory.

In app.py, find where the embedding model is loaded during startup (inside the lifespan function). It currently looks like:

    embed_model_id = os.getenv("EMBED_MODEL_ID", "sentence-transformers/all-MiniLM-L6-v2")
    logger.info(f"Loading embedding model: {embed_model_id}")
    tokenizer = AutoTokenizer.from_pretrained(embed_model_id)
    embedder = AutoModel.from_pretrained(embed_model_id)

This already works with local paths — sentence-transformers and AutoModel both accept a local directory path. No code change needed, just the environment variable.

But also update app.py to add the /embed endpoint if it hasn't been added yet (from the Java service refactor):

    @app.post("/embed")
    async def generate_embedding_endpoint(request: dict):
        """Return raw embedding vector for a text string.
        Used by the Java search service to delegate embedding generation."""
        text = request.get("text", "").strip()
        if not text:
            raise HTTPException(status_code=400, detail="text field is required")
        
        embedding = get_embedding(text)
        
        return {
            "embedding": embedding,
            "dimensions": len(embedding),
            "model": os.getenv("EMBED_MODEL_ID", "sentence-transformers/all-MiniLM-L6-v2")
        }

CHANGE 2: Update the ECS task definition to point to the local model path.

In the task definition for llm-inference-task, change the EMBED_MODEL_ID environment variable:

    FROM: {"name": "EMBED_MODEL_ID", "value": "sentence-transformers/all-MiniLM-L6-v2"}
    TO:   {"name": "EMBED_MODEL_ID", "value": "/app/embedding-finetune/fine-tuned-all-MiniLM-L6-v2-amazon"}

CHANGE 3: Update the Dockerfile to include the fine-tuned model in the image.

In ~/llm-inference/Dockerfile, add a COPY line to include the fine-tuned model directory:

    # Copy the fine-tuned embedding model
    COPY embedding-finetune/fine-tuned-all-MiniLM-L6-v2-amazon/ ./embedding-finetune/fine-tuned-all-MiniLM-L6-v2-amazon/

This should be placed after the main COPY of app files. The model directory is about 80MB and includes:
- config.json, config_sentence_transformers.json
- model.safetensors (the fine-tuned weights)
- tokenizer.json, tokenizer_config.json
- special_tokens_map.json

CHANGE 4: Rebuild and redeploy the Python service:

    cd ~/llm-inference
    docker build --platform linux/amd64 --no-cache -t llm-inference:local .
    docker tag llm-inference:local 717914742237.dkr.ecr.us-east-1.amazonaws.com/llm-inference:latest
    aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 717914742237.dkr.ecr.us-east-1.amazonaws.com
    docker push 717914742237.dkr.ecr.us-east-1.amazonaws.com/llm-inference:latest
    
    # Update task definition with new EMBED_MODEL_ID
    aws ecs register-task-definition --cli-input-json file://task-definition-v3-performance.json --region us-east-1
    
    # Force new deployment
    aws ecs update-service --cluster llm-cluster --service llm-inference-service --task-definition llm-inference-task --force-new-deployment --region us-east-1

CHANGE 5: Verify the model loaded correctly:

    # Wait 2-3 minutes for deployment, then:
    curl http://$ALB_DNS/health
    # Should show "embedder_loaded": true

    # Check that the /embed endpoint returns the fine-tuned model name:
    curl -s -X POST http://$ALB_DNS/embed \
      -H "Content-Type: application/json" \
      -d '{"text": "running shoes"}' | python3 -c "import sys,json; d=json.load(sys.stdin); print(f'Model: {d[\"model\"]}, Dims: {d[\"dimensions\"]}')"
    # Should print the local fine-tuned model path

    # Check that the Java service also reflects the new model:
    curl http://$ALB_DNS/api/info
    # The embedding_model field should now show the fine-tuned model path
    # (because the Java service's EmbeddingService reads the model name from Python's response)

The key insight: because of the Option A architecture, ONE deployment of the Python service
automatically updates embedding generation for BOTH the Python endpoints AND the Java service.
No ONNX export needed. No Java rebuild needed. Single source of truth.
```

---

## PROMPT 10 — Generate the Final Evaluation Report Document

**Interview Explanation:** *This generates a professional markdown report that documents the entire fine-tuning process, the metrics, and the conclusions. This is the artifact I'd share in an interview or include in project documentation to show the work I did.*

```
Create a script called generate_report.py that:

1. Loads data/baseline_results.json and data/finetuned_results.json (or data/comparison_report.json)

2. Generates a markdown file called EVALUATION_REPORT.md with this structure:

   # Embedding Model Fine-Tuning Evaluation Report

   ## Executive Summary
   (One paragraph: what was done, the primary metric improvement, and the conclusion)

   ## System Context
   - Database: PostgreSQL + pgvector on RDS (1,013 Amazon products)
   - Base Model: sentence-transformers/all-MiniLM-L6-v2 (384 dimensions)
   - Search Pipeline: Java Spring Boot → delegates to Python /embed endpoint → pgvector cosine similarity (HNSW)

   ## Methodology

   ### Training Data Generation
   - Used Claude (claude-sonnet-4-20250514) to generate synthetic search queries
   - 4 queries per product × 1,013 products = ~4,052 total pairs
   - 80/20 train/test split (seed=42)
   - Query diversity: specific, general, need-based, attribute-focused

   ### Fine-Tuning Configuration
   - Loss: MultipleNegativesRankingLoss (in-batch negatives)
   - Epochs: 3
   - Batch size: 32
   - Learning rate: 2e-5 with 10% warmup
   - Hardware: {CPU/GPU} ({model name})
   - Training time: {X} minutes

   ### Evaluation Protocol
   - Evaluator: sentence_transformers InformationRetrievalEvaluator
   - Corpus: Full 1,013 product catalog
   - Test queries: {N} held-out synthetic queries
   - Metrics: NDCG@10 (primary), MRR@10, Recall@k, Accuracy@k

   ## Results

   ### Metric Comparison Table
   (Formatted table with all metrics, baseline, fine-tuned, delta, % change)

   ### Key Findings
   - NDCG@10 improved from X to Y (+Z%)
   - Accuracy@1 improved from X to Y (correct product ranked first Z% more often)
   - Average similarity scores increased, indicating tighter query-product alignment

   ## Search Quality Examples
   (Include the 5 side-by-side query comparisons if available)

   ## Deployment
   - Re-embedded all 1,013 products with fine-tuned model
   - Updated Python inference service to load fine-tuned model from local path
   - Java service automatically uses improved embeddings via /embed delegation (Option A architecture)
   - No ONNX export needed — single source of truth in Python
   - No schema changes needed (384 dimensions preserved)
   - HNSW indexes automatically updated

   ## Limitations and Future Work
   - Training data is synthetic (generated by LLM), not real user search logs
   - With real click-through data, further improvement likely
   - Could explore hard negative mining for finer-grained distinction
   - Could add Matryoshka Representation Learning for storage efficiency
   - Could add cross-encoder reranking as a second stage for highest-precision queries

3. If the results files don't exist yet, generates the report with placeholder values and a note: "(Run the evaluation scripts to populate these values)"

4. Prints: "Report generated: EVALUATION_REPORT.md"

The script should be runnable as: python generate_report.py
```

---

## Execution Order Summary

Run these scripts in order. The entire process takes about 30-45 minutes.

| # | Script | Time | What It Does |
|---|--------|------|-------------|
| 1 | `01_extract_products.py` | ~5s | Pulls 1,013 products from PostgreSQL |
| 2 | `02_generate_training_data.py` | ~15 min | Calls Claude API to generate ~4,000 synthetic query pairs |
| 3 | `03_baseline_evaluation.py` | ~30s | Measures NDCG@10 before fine-tuning |
| 4 | `04_fine_tune.py` | ~15 min | Fine-tunes the model, evaluates after each epoch |
| 5 | `05_evaluate_improvement.py` | ~60s | Full comparison report: baseline vs fine-tuned |
| 6 | `07_compare_search_results.py` | ~5s | Captures BEFORE results from live API |
| 6b | `cp data/live_search_results.json data/live_search_results_BEFORE.json` | — | Save the before snapshot |
| 7 | `06_re_embed_products.py` | ~30s | Updates all embeddings in PostgreSQL |
| 8 | `07_compare_search_results.py` | ~5s | Shows AFTER results + comparison |
| 9 | Update `app.py` + Dockerfile + task def | ~5 min | Deploy fine-tuned model to Python service (Java gets it automatically) |
| 10 | `generate_report.py` | ~2s | Creates the professional evaluation report |

---

## Interview Talking Points — How to Explain Each Concept

### "What is NDCG and why did you choose it?"
*NDCG stands for Normalized Discounted Cumulative Gain. It's the standard ranking quality metric in information retrieval. Unlike simple hit-rate which just checks if the correct result appears anywhere in the top-k, NDCG considers the position. A correct result at rank 1 contributes more to the score than one at rank 8, because the "discount" is logarithmic in the rank position. It's normalized to a 0-1 scale. I chose it because it's the primary metric used in BEIR and MTEB benchmarks, and it directly reflects the user experience — higher NDCG means the best products appear first.*

### "Why MultipleNegativesRankingLoss instead of other loss functions?"
*MNR loss (also called InfoNCE) is ideal when you only have positive pairs — a query and its correct product — but no explicit negative examples. The clever trick is "in-batch negatives": for a batch of 32 pairs, each query's positive product is treated as a negative for all other queries in the batch. This gives 31 free negatives per query. Alternatives like TripletLoss require explicit negative examples, and ContrastiveLoss requires labeled positive/negative pairs. With synthetic data where I only have positives, MNR is the natural choice. It's also what most state-of-the-art embedding models use in their training.*

### "Why synthetic data instead of real search logs?"
*We don't have real user search logs because this is a new system. In production, you'd use click-through data: when a user searches "running shoes" and clicks on the adidas Racer, that's a positive signal. Since we don't have that, I use an LLM to generate realistic queries — "what would a customer type when looking for this product?" This is a well-established technique: the LlamaIndex and Sentence Transformers documentation both recommend it, and published results show 5-10% NDCG improvement from synthetic data alone. In a real system, I'd supplement with actual user behavior data over time.*

### "What happens to the embeddings during fine-tuning?"
*The model has about 22 million parameters. During fine-tuning, all parameters are updated via gradient descent. The 6 transformer layers learn to produce slightly different hidden representations, and the pooling layer aggregates them differently. The result is that the 384-dimensional output vectors shift in the embedding space: queries like "budget electronics" move closer to cheap electronics products and farther from expensive ones. The dimensionality doesn't change (still 384), and the tokenizer is unchanged — only the transformer weights are updated. This means I can swap in the new model without changing the database schema, the vector indexes, or the API contracts.*

### "How do you know the improvement is real and not just overfitting to synthetic queries?"
*Three safeguards: First, I use a held-out test set — 20% of queries never seen during training. The evaluator only uses these unseen queries. Second, I evaluate against the full 1,013-product corpus, not just the products that appeared in training. Third, I run qualitative checks with manually crafted queries like "gift for someone who loves cooking" that are very different from the synthetic training queries. If those also show improved results, the model has genuinely learned better product-domain semantics rather than memorizing the training set.*
